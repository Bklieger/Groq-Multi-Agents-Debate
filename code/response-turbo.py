import openai
import json
import argparse
from tqdm import tqdm
import time
import tiktoken

# gpt-3.5-turbo may change over time. Returning a surplus of num tokens.
model2max_context = {
    "gpt-4-0314": 8000, # max 8196
    "gpt-3.5-turbo-0301": 3900, # max 4096
    "gpt-3.5-turbo": 3900, # max 4096
}
    
def num_tokens_from_string(string: str, model_name: str) -> int:
    """Returns the number of tokens in a text string."""
    encoding = tiktoken.encoding_for_model(model_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens

def get_response(messages, max_token, api_key=None, temperature=0, model_name=None):
    response = openai.ChatCompletion.create(
            model=model_name,
            messages=messages,
            temperature=temperature,
            top_p=1.0,
            max_tokens=max_token,
            api_key=api_key
        )
    return response['choices'][0]['message']['content']

def set_meta_prompt(prompt: list, meta_prompt: str):
    """Set the meta_prompt

    Args:
        meta_prompt (str): the meta prompt
    """
    prompt.append({"role": "system", "content": f"{meta_prompt}"})
    return prompt

def add_event(prompt: list, event: str):
    """Add an new event in the memory

    Args:
        event (str): string that describe the event.
    """
    prompt.append({"role": "user", "content": f"{event}"})
    return prompt

def add_memory(prompt: list, memory: str):
    """Monologue in the memory

    Args:
        memory (str): string that generated by the model in the last round.
    """
    prompt.append({"role": "assistant", "content": f"{memory}"})
    return prompt

def parse_args():
    parser = argparse.ArgumentParser("", formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument("-i", "--input", type=str, required=True,
        help="Input file path")
    parser.add_argument("-o", "--output", type=str, required=True,
        help="Output file path")
    parser.add_argument("-k", "--api-key", type=str, required=True,
        help="OpenAI api key")
    parser.add_argument("-m", "--model-name", type=str, default="gpt-3.5-turbo",
        help="Model name")
    parser.add_argument("--temperature", type=float, default=0,
        help="Sampling temperature")

    return parser.parse_args()

def main():
    args = parse_args()

    in_file_path = args.input
    out_file_path = args.output
    api_key = args.api_key
    model_name = args.model_name
    temperature = args.temperature

    inputs = open(in_file_path, "r").readlines()
    inputs = [json.loads(l) for l in inputs]

    with open(out_file_path, 'w', encoding='utf-8') as out_file:
        for id, input in enumerate(tqdm(inputs)):
            prompt = []
            if "meta_prompt" in input:
                prompt = set_meta_prompt(prompt, input["meta_prompt"])
            
            if "prompt" in input:  
                input_content = input["prompt"] + input["input"]
            else:
                input_content = input["input"]
            prompt = add_event(prompt, input_content)

            len_prompt = sum([num_tokens_from_string(m["content"], model_name) for m in prompt])
            max_token = model2max_context[model_name] - len_prompt
            response = get_response(prompt, max_token, api_key, temperature, model_name)

            input.update({"output": response})
            json_str = json.dumps(input, ensure_ascii=False)
            print(json_str, file=out_file, flush=True)
            # time.sleep(20)
            
if __name__ == "__main__":
    main()
